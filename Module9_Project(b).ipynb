{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MancheelaNithinkumar/FMML_Project_and_Labs/blob/main/Module9_Project(b).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 9 Project (b): Convolutional Neural Networks Project\n",
        "\n",
        "## Module coordinator: Kushagra Agarwal\n",
        "\n"
      ],
      "metadata": {
        "id": "LF8nIY5yEw3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://hub.packtpub.com/wp-content/uploads/2018/04/iStock-851960058-696x464.jpg\" width=850px/>"
      ],
      "metadata": {
        "id": "2EIAlTLKPQj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, you will understand how you can perform emotion recognition using CNNs in a step-by-step manner. To make your task easier, we provide you the starter code for the project. It is expected that you should try to understand the project statement properly and perform the tasks in sequence. We will be using Pytorch framework for the implementation. You need to fill in the missing code parts to achieve a particular task. At the end, you will have a basic implementation ready for an emotion detection application.\n",
        "\n",
        "Basic steps involved in Emotion Recognition:\n",
        "- Face detection\n",
        "- Building classifier\n",
        "- Classifying emotions\n",
        "\n",
        "We will use a popular FER2013 dataset for this project."
      ],
      "metadata": {
        "id": "jL7zt_6bPNXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Explore the dataset\n",
        "The dataset contains 48 x 48 grayscale facial images of faces.The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)."
      ],
      "metadata": {
        "id": "hpBbqZ39FF_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.researchgate.net/profile/Chaudhary-Aqdus/publication/349055345/figure/fig3/AS:987834383085568@1612529478973/FER-2013-sample-images-for-facial-emotion-recognition.jpg\" width=650px/>"
      ],
      "metadata": {
        "id": "c9UGeEuTPr4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "kXy7bFuIFPA_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hh_RSwSCEnrZ"
      },
      "outputs": [],
      "source": [
        "# We have imported the necessary packages here. Feel free to import anything more you need!\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import dlib\n",
        "import cv2\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and load dataset"
      ],
      "metadata": {
        "id": "o9BUAlmDFeEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1YrNrok2Z1udWWIpejXIdLk7duUq87s0N\n",
        "!unzip fer2013.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaPP5dHSFWgM",
        "outputId": "edca4955-0f6e-41c1-f255-440127910285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1YrNrok2Z1udWWIpejXIdLk7duUq87s0N\n",
            "From (redirected): https://drive.google.com/uc?id=1YrNrok2Z1udWWIpejXIdLk7duUq87s0N&confirm=t&uuid=8f031f02-cfb9-46a0-885c-9bbef3a11a0d\n",
            "To: /content/fer2013.csv.zip\n",
            "100% 101M/101M [00:01<00:00, 54.6MB/s] \n",
            "Archive:  fer2013.csv.zip\n",
            "replace fer2013.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset csv using pandas package. It displays the data in tabular form\n",
        "emotion_data = pd.read_csv('./fer2013.csv')\n",
        "print(emotion_data)"
      ],
      "metadata": {
        "id": "9LOSUCeZFglQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class dictionary for dataset\n",
        "classes = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}"
      ],
      "metadata": {
        "id": "petzLP_HFjNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize a few images from dataset"
      ],
      "metadata": {
        "id": "nyWdEqz3FpCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,4))\n",
        "for i in range(10):\n",
        "  ax = plt.subplot(2,5,i+1)\n",
        "  # This is how we access ith row in 'pixels' column in the dataset table\n",
        "  img = emotion_data.iloc[i]['pixels'].split(' ') # Converting into array of ints\n",
        "  img = np.array(img).astype(int)\n",
        "\n",
        "  # Labels for our dataset\n",
        "  label = int(emotion_data.iloc[i]['emotion'])\n",
        "  ax.imshow(img.reshape((48,48)), cmap='gray')\n",
        "  ax.set_title(classes[label])\n",
        "  ax.set_axis_off()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nnuUwMwaFoWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names, counts = np.unique(emotion_data['Usage'].to_numpy(), return_counts=True)\n",
        "print('Number of samples in {} = {}'.format(names[0], counts[0])) #testset\n",
        "print('Number of samples in {} = {}'.format(names[1], counts[1])) #valset\n",
        "print('Number of samples in {} = {}'.format(names[2], counts[2])) #trainset"
      ],
      "metadata": {
        "id": "jOYEnyhqFquh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of class labels"
      ],
      "metadata": {
        "id": "exip5hjXF9uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot bar chart showing number of samples per class in the train set\n",
        "temp_train = emotion_data.loc[emotion_data['Usage'] == 'Training']\n",
        "df_temp_train = temp_train.sort_values(by = \"emotion\", inplace = False)\n",
        "fig = plt.figure(figsize = (7, 5))\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.set_title(\"Count of each Emotion in Train Data\", fontsize = 20)\n",
        "sns.countplot(x = \"emotion\", data = df_temp_train)\n",
        "plt.grid()\n",
        "for i in ax.patches:\n",
        "    ax.text(x = i.get_x() + 0.2, y = i.get_height()+1.5, s = str(i.get_height()), fontsize = 20, color = \"grey\")\n",
        "plt.xlabel(\"Classes\"+ str(classes))\n",
        "plt.ylabel(\"Count\", fontsize = 15)\n",
        "plt.tick_params(labelsize = 15)\n",
        "plt.xticks(rotation = 40)\n",
        "plt.show()\n",
        "\n",
        "### Task: Similarly, write the code below to plot the charts for remaining two sets also."
      ],
      "metadata": {
        "id": "9fu3LIjCFzva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the imbalance in the data through above graphs."
      ],
      "metadata": {
        "id": "RgyJVkVeGJiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Face detection: Many applications involving facial images as input data require face detection in the pipeline at this step. Here, we localise the face in the given image removing the irrelevant parts, making the face centered and occupying most of the part in the image. As mentioned earlier, our dataset already has more or less centered faces, so we will skip this step for now but when using some other dataset or using your own images (eg. from webcam) as you will do later, you can do this step to get a proper cropped face from the image."
      ],
      "metadata": {
        "id": "tGzXGpFgHcED"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKqBdliUNhiS"
      },
      "source": [
        "## Task 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QmcaX7yNsMf"
      },
      "source": [
        "### Creating train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eBsC7i2-NTI"
      },
      "source": [
        "X_train, y_train = [], []\n",
        "X_val, y_val = [], []\n",
        "X_test, y_test = [], []\n",
        "\n",
        "for index, row in emotion_data.iterrows():\n",
        "  k = row['pixels'].split(\" \")\n",
        "\n",
        "  if row['Usage'] == 'Training':\n",
        "    X_train.append(np.array(k))\n",
        "    y_train.append(row['emotion'])\n",
        "\n",
        "  # Similarly write the conditions for test and val splits here\n",
        "  ###### YOUR CODE HERE  ######\n",
        "\n",
        "\n",
        "\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_val, y_val = np.array(X_val), np.array(y_val)\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "\n",
        "print('Training set shape: ', X_train.shape, y_train.shape)\n",
        "print('Validation set shape: ', X_val.shape, y_val.shape)\n",
        "print('Testing set shape: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pvWe82a_KIO"
      },
      "source": [
        "# To get data between 0 and 1\n",
        "X_train = X_train.astype(float) / 255.\n",
        "X_test = X_test.astype(float) / 255.\n",
        "X_val = X_val.astype(float) / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_A4SIWRYvVd"
      },
      "source": [
        "We will define a dataset wrapper over Pytorch Dataset class which takes in the numpy arrays we created and returns a sample with required preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkU6ZNlIl_uw"
      },
      "source": [
        "class Fer2013Dataset(Dataset):\n",
        "  def __init__(self, x, y, transforms=None):\n",
        "    self.x = x.reshape((-1, 48, 48))\n",
        "    self.y = y\n",
        "    self.transforms= transforms\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img, y = self.x[index], self.y[index]\n",
        "\n",
        "    if self.transforms is not None:\n",
        "        img = self.transforms(img)\n",
        "    return img, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfyIAW5tYZXf"
      },
      "source": [
        "batch_size=32\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Create tensor dataset from above tensors\n",
        "train_dataset = Fer2013Dataset(X_train, y_train, transforms=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "val_dataset = Fer2013Dataset(X_val, y_val, transforms=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "test_dataset = Fer2013Dataset(X_test, y_test, transforms=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVeKhcxOaOi-"
      },
      "source": [
        "## Task 3: Building a CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-fjjr6UZ1sc"
      },
      "source": [
        "# Define your CNN architecture here\n",
        "# To start with, let's say you can create a model with 4 relu-activated convs,\n",
        "# each followed by a pooling layer. Then, you can add 2-3 fully connected layers\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        #### YOUR CODE HERE  ####\n",
        "\n",
        "    def forward(self,x):\n",
        "        #### YOUR CODE HERE  ####\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh2oKndcbVKa"
      },
      "source": [
        "# Device (CPU/GPU)\n",
        "device = 'cpu' #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the CNN\n",
        "model = Net().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training/Testing functions"
      ],
      "metadata": {
        "id": "JffKONlJKQXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, loss_func, optimizer, num_epochs):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      # Average loss and acc values\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ],
      "metadata": {
        "id": "S6BE_5aPF_eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # The class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network: %d %%' % (\n",
        "      100 * correct / total))"
      ],
      "metadata": {
        "id": "w2BCd3JOJHnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcBGf4oZdRYD"
      },
      "source": [
        "## Task 4: Training & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skrXHHTlfDNj"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKRqaFn6dU3t"
      },
      "source": [
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer with learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "history = train(model, train_loader, criterion, optimizer, num_epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, loss_func, optimizer, num_epochs):\n",
        "    # Training mode\n",
        "    model.train()\n",
        "\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0\n",
        "        running_acc = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # clear gradients for this training step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(images)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_func(output, labels)\n",
        "\n",
        "            # Backpropagation, compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Apply gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # Running loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # indices of max probabilities\n",
        "            _, preds = torch.max(output, dim=1)\n",
        "\n",
        "            # Calculate number of correct predictions\n",
        "            correct = (preds == labels).sum().item()\n",
        "            running_acc += correct\n",
        "\n",
        "        # Calculate epoch loss and accuracy\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_acc.append(epoch_acc)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc*100:.2f}%')\n",
        "\n",
        "    return train_losses, train_acc"
      ],
      "metadata": {
        "id": "NG1wb_bFKvv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIc4E7dDd-RJ"
      },
      "source": [
        "### Evaluate your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofh6NYJe_AfQ"
      },
      "source": [
        "# Visualize top K predictions\n",
        "def visualize_prediction(image, model, k=3):\n",
        "  model.eval()\n",
        "\n",
        "  preds = model(image.unsqueeze(1).float())\n",
        "\n",
        "  topk = torch.topk(preds, k, dim=1)\n",
        "  topk = topk.indices.numpy()\n",
        "  print('Top {} Predictions'.format(k))\n",
        "  for i in range(3):\n",
        "    print('{}) {}'.format(i+1, classes[topk[0][i]]))\n",
        "\n",
        "  plt.imshow(image[0].numpy(), cmap='gray')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HexADTCBiFU"
      },
      "source": [
        "# Visualize top K predictions\n",
        "def visualize_prediction(image, model, k=3):\n",
        "  model.eval()\n",
        "\n",
        "  # Reshape the image to have 3 channels\n",
        "  image = image.repeat(3, 1, 1).unsqueeze(0).float()\n",
        "\n",
        "  preds = model(image)\n",
        "\n",
        "  topk = torch.topk(preds, k, dim=1)\n",
        "  topk = topk.indices.numpy()\n",
        "  print('Top {} Predictions'.format(k))\n",
        "  for i in range(3):\n",
        "    print('{}) {}'.format(i+1, classes[topk[0][i]]))\n",
        "\n",
        "  plt.imshow(image[0][0].numpy(), cmap='gray') # Display only the first channel\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NN8ebRBeZVL"
      },
      "source": [
        "def test_model(model, testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  true_labels = []  # List to store true labels\n",
        "  predicted_labels = []  # List to store predicted labels\n",
        "\n",
        "  # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # The class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        true_labels.extend(labels.tolist())  # Append true labels to the list\n",
        "        predicted_labels.extend(predicted.tolist())  # Append predicted labels to the list\n",
        "\n",
        "  print('Accuracy of the network: %d %%' % (\n",
        "      100 * correct / total))\n",
        "  return true_labels, predicted_labels  # Return both lists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load pretrained ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all layers initially\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace final fully connected layer (ResNet18 has 512 output features before FC)\n",
        "num_classes = 10  # whatever your number of target classes is\n",
        "model.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "# Now only the final layer's parameters will be trained\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "ajML6_T_7bbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# BONUS TASK\n",
        "\n",
        "### How can you improve the performance of your model given that the number of datapoints is fixed?\n",
        "\n",
        "##### Hint: A very simple fix (discussed in Lab 2) is to use a pretrained CNN model. The pretrained model could be trained on any dataset (eg Imagenet) and the first few layers of the same can be directly used for this task.\n",
        "\n",
        "### You are encouraged to try out different pretrained models like ResNet/VGG/AlexNet and see how the performance improves. Do all the models result in similar accuracy?"
      ],
      "metadata": {
        "id": "FwAjEp34QCsP"
      }
    }
  ]
}